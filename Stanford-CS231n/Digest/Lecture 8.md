# 第八讲：注意力机制与Transformer

## 课程开场与回顾

大家好，欢迎回到今天的第八讲。我们今天要讨论**注意力机制（Attention）和Transformer模型**，我觉得这个主题真的非常有趣。

快速回顾一下上次的内容：我们讲到了**循环神经网络（Recurrent Neural Networks, RNN）**。循环神经网络是一种新的神经网络架构，用于处理序列数据。特别是我们看到，通过处理序列数据，神经网络使我们能够解决之前无法处理的全新类型问题，这是卷积网络之前做不到的。

之前我们一直在考虑**一对一（one-to-one）**的问题，比如输入一张图像，然后输出一个分类结果。但当我们能够突破图像处理，转向序列数据时，就能让我们解决大量新类型的问题：
- **一对多（one-to-many）**问题：图像描述生成（image captioning），输入一张图像，输出该图像的文本描述序列
- **多对一（many-to-one）**问题：输入一系列帧，输出这些帧的分类结果
- 以及这类问题的其他变种

我们看到，采用这些更复杂的神经网络架构，不仅在架构上更有趣，还能让我们解决传统前馈神经网络无法处理的问题。

## 今天的内容

今天我们将在此基础上继续，讲座将介绍两个新内容：

1. **注意力机制（Attention）**：一种全新的神经网络基础单元，本质上处理一组向量（a set of vectors）
2. **Transformer模型**：一种新的神经网络架构，核心是自注意力机制（self-attention）

这里有个提示：**Transformer基本上是当今深度学习几乎所有问题的解决方案**。你今天在实际应用中看到的任何大型应用——无论是图像分类、图像生成、文本生成、文本分类、处理音频——基本上所有现代大型神经网络，那些在大量数据上训练、由大公司部署的顶尖模型，今天几乎全部都是Transformer架构。这真是太令人兴奋了！

## Transformer的发展历程

虽然Transformer已成为这一领域的标杆，如今被广泛应用于各种任务的主流架构，但它们其实有着相对较长的发展历史。观察这些领域的发展过程确实很有趣。

回想当初Transformer刚出现的时刻，你可能会觉得这应该是重大突破，是引发重大变革的转折点，是全新的架构和创新事物。但实际上并没有显得那么轰动。虽然Transformer架构诞生于某个时刻，但关于自注意力机制和多方式注意力应用的概念，实际上早已在该领域存在多年。

特别是**注意力机制和自注意力机制，它们实际上源自循环神经网络**。因此，我们将从这里开始讲解并阐述这些问题，重现这些理念的发展历程。出于这个原因，为了介绍Transformer，我们需要回溯，简要回顾循环神经网络的相关概念。

## 序列到序列问题：机器翻译

让我们思考**序列到序列（Sequence-to-Sequence）的翻译问题**。我们需要输入一个英文单词序列，然后输出另一个目标语言（比如意大利语）的单词序列。

因为我们不能假设两种语言词汇存在一一对应关系——英文句子的词数可能与意大利语句子的词数不同，甚至顺序可能完全不同——这正是序列处理算法的理想应用场景。

### RNN编码器-解码器架构

使用循环神经网络处理序列到序列问题，这一理念可追溯至2014年，甚至更早。人们已用循环神经网络处理序列超过十年。

处理序列到序列问题的基本架构是：

**编码器（Encoder）**：
- 编码器是一个循环神经网络
- RNN的核心是递归应用于两个输入的函数：
  - 当前时间步的输入 $x_t$
  - 前一时间步的隐藏状态 $h_{t-1}$
- 公式：$h_t = f_W(x_t, h_{t-1})$

我们用一个简短的例子："we see the sky"。句子中的每个单词都会被处理，每个单词对应循环神经网络的一次迭代。

这个编码器RNN需要处理输入序列中的所有单词，并总结输入句子的内容，以便将其翻译成目标语言。更具体地说，在处理完输入序列中的所有单词后，我们**将整个输入序列的内容总结为一个向量，称为上下文向量（context vector）$c$**。

通常有几种不同的方法来构造这个上下文向量。一个简单的做法是，**这个上下文向量基本上就是编码器的最后一个隐藏状态**。核心思想是，由于循环神经网络的结构，最后一个隐藏状态包含了整个输入序列的信息。我们可以将最后一个隐藏状态视为总结或编码整个输入序列的所有信息。

**解码器（Decoder）**：
- 为了将输入序列翻译成另一种语言的输出序列，我们使用第二个RNN称为解码器
- 解码器通常具有相同架构但不同的权重矩阵，具有不同的学习参数
- 公式：$s_t = g_U(y_{t-1}, s_{t-1}, c)$

每个时间步解码器接收三个输入：
- $y_{t-1}$：输出序列前一时间步的token
- $s_{t-1}$：输出序列的前一隐藏状态
- $c$：上下文向量，总结整个输入序列

然后我们逐个生成输出序列中的词语。比如意大利语的"vediamo"（我们看到）、"il"（the）、"cielo"（天空）。

### RNN的瓶颈问题

但这里可能存在一个问题：**输入序列与输出序列之间存在通信瓶颈**。

输入序列与输出序列唯一的信息传递方式是通过上下文向量 $c$，而 $c$ 是一个**固定长度的向量**。该向量的尺寸在设置网络时固定，取决于我们设定的循环神经网络大小。也许 $c$ 是一个包含128或1024个浮点数的固定长度向量。

这对于短序列如"we see the sky"可能还算合理，可以将所需信息全部总结到这个固定向量中。但如果不是翻译四个单词，而是整个段落、整本书或整个数据集，此时我们将遇到瓶颈。随着输入序列规模扩大，最终将不再合理要求网络将整个输入序列压缩成一个单一固定长度的向量。

## 注意力机制的引入

### 解决方案

**解决方法是：不要通过固定长度向量限制网络，而是改变循环神经网络的架构。**

直观来说，我们不想在输入和输出之间强制一个固定长度向量的瓶颈。相反，在处理输出序列时，我们要**让模型能够回看（look back）输入序列**。每次生成输出向量时，我们要给网络机会回看整个输入序列。这样做就不会有瓶颈，能够处理更长的序列，模型架构应该会表现更好。

这就是**注意力机制和Transformer的灵感来源**。今天我们看到的所有先进技术，都源于解决循环神经网络中的瓶颈问题。

### 注意力机制的实现

现在看看如何实现这个直觉：赋予我们的循环神经网络在每个时间步回看输入序列的能力。

**编码器部分保持不变**：
- 编码器神经网络无需改动
- 我们仍需为输出序列设置初始隐藏状态，获得初始解码器状态 $s_0$

**关键步骤：**

1. **计算对齐分数（Alignment Scores）**：
   - 通过比较初始解码器状态 $s_0$ 与输入序列每个词的隐藏状态，计算标量值
   - 在此例中输入序列有四个词，需计算四个对齐分数：$e_{11}, e_{12}, e_{13}, e_{14}$
   - 每个分数表示该解码器状态与输入序列中的词的匹配程度
   - 实现方法：使用一个简单的线性层 $f_{att}$
   - 公式：$e_{t,i} = f_{att}(s_{t-1}, h_i)$
   - 这个线性层接收解码器隐藏状态与其中一个编码器隐藏状态，将这两个进行拼接形成向量，然后应用线性变换将其压缩为标量

2. **应用Softmax函数**：
   - 这些标量对齐分数是无界的任意实数值
   - 我们通过应用softmax函数来添加结构，防止数值爆炸
   - Softmax将任意分数向量转换为概率分布
   - 输出：$a_{11}, a_{12}, a_{13}, a_{14}$
   - 每个元素在0到1之间且总和为1
   - 这是输入token上的概率分布，表示给定该解码器隐藏状态时的注意力权重

3. **计算上下文向量**：
   - 使用注意力分数对编码器隐藏状态取加权线性组合
   - 公式：$c_1 = \sum_i a_{1i} h_i$
   - 该向量（用紫色表示）将总结编码器序列的信息，以某种由这些注意力权重调节的方式

4. **传递给解码器RNN**：
   - 将上下文向量 $c_1$ 与输出序列的第一个token连接
   - 传递给循环单元以获取解码器RNN的下一个隐藏状态和第一个输出token

### 注意力机制的直觉

直觉是：**这个上下文向量能够"关注"或查看输入序列的不同部分**，由输出RNN此时想要查看的部分调节。

例如，在生成意大利语单词"vediamo"（we see）时，网络可能需要回看输入序列中的"we see"这两个词。因此我们可能期望网络会给这两个词更高的注意力权重（如 $a_{11}=a_{12}=0.45$），而对"the sky"不关心（$a_{13}=a_{14}=0.05$）。

**关键点**：这一切都是**可微的**！我们不需要监督网络，不需要告诉它输入序列中哪些词是必需的。相反，这只是一个由可微操作组成的大型计算图，所有这些都可以通过梯度下降端到端学习。网络正在尝试预测输出序列中的正确token，在此过程中，它将**自行学习如何关注输入序列的不同部分**。

如果我们必须手动监督并告诉网络两者之间的对齐关系，获取这种训练数据将会非常困难。

### 重复过程

这个过程对解码器的每个时间步重复：

- 对于第二个时间步，基于 $s_1$，我们再次计算新的对齐分数、注意力权重和上下文向量 $c_2$
- 使用 $c_2$ 生成下一个输出token（如"il"）
- 对每个解码器时间步重复此过程

现在我们**不再通过单一固定长度向量限制输入序列**，而是有了新的机制：在解码器每个时间步，网络回顾整个输入序列，重新总结输入序列，以便生成当前解码步骤的上下文向量。

### 注意力权重的可视化

注意力机制的另一个酷炫之处是：我们可以**观察网络关注的内容**。

通过观察网络预测的注意力权重，我们能感知到网络在解决问题时关注的内容。这为我们提供了一种理解神经网络处理过程的方法。

**示例：英语到法语翻译**
- 输入："The agreement on the European Economic Area was signed in August 1992."
- 输出："L'accord sur la zone économique européenne a été signé en août 1992."

可视化注意力权重矩阵：
- **对角结构**：意味着输入与输出序列之间存在一对一对应和顺序对应
- 前四个词和最后几个词显示对角结构
- **非对角结构**：中间部分显示词语顺序的差异（英语vs法语）
- 网络自行发现了英法词语的不同顺序
- 有时出现2×2的网格结构，表示两个词之间的对应关系不是完全分离的

**网络自行完成了所有推导**，通过大量数据训练和计算资源投入。这非常酷！

### 历史背景

这是来自2015年的论文：**Bahdanau et al, "Neural machine translation by jointly learning to align and translate", ICLR 2015**

这篇论文实际上在ICLR 2025获得了时间测试奖（Test of Time Award）亚军。这篇论文多年来影响深远。

## 将注意力泛化为通用操作符

事实上这里有一个更通用的概念，隐藏着更通用的操作符。我们从修复循环神经网络的角度切入这个问题，但用于修复RNN的机制实际上是一种**通用且有趣且强大的机制**。

现在我们想提取出这个机制，**将注意力机制与循环神经网络分离**。结果发现注意力机制非常有用，成为神经网络中强大的计算原语，即使去掉循环神经网络部分。

### 注意力层的抽象

让我们思考注意力机制的作用：

**输入**：
- **查询向量（Query vectors）**：解码器RNN的隐藏状态，我们试图生成输出的内容
- **数据向量（Data vectors）**：编码器RNN的隐藏状态，需要总结的数据

**计算过程**（单个查询向量 $q$ [维度 $D_Q$]，数据向量 $X$ [维度 $N_X \times D_X$]）：

1. **相似度（Similarities）**：$e$ [$N_X$]
   $$e_i = f_{att}(q, X_i)$$

2. **注意力权重（Attention weights）**：
   $$a = \text{softmax}(e) \quad [N_X]$$
   其中 $0 < a_i < 1$ 且 $\sum_i a_i = 1$

3. **输出向量（Output vector）**：
   $$y = \sum_i a_i X_i \quad [D_X]$$

**注意力操作的作用**：接收一个查询向量，回到输入数据向量，以新的方式汇总数据向量生成输出向量。

### 泛化1：使用缩放点积相似度

我们要做的第一个泛化是**让相似度函数变得更简单**。

输入两个向量并输出标量的最简单可能函数是什么？**点积（Dot Product）**！

结果发现点积作为相似度分数已经足够好用。

**缩放的必要性**：
点积与softmax之间存在奇怪的相互作用。当向量维度增大时：
- 假设我们有维度为10的全一向量 vs 维度为100的全一向量
- 回忆 $a \cdot b = |a||b|\cos(\theta)$
- 如果 $a$ 和 $b$ 是常数向量，维度为 $D$，则 $|a| = (\sum_i a^2)^{1/2} = a\sqrt{D}$
- 更高维度导致更大的点积值，softmax后的概率更压缩，可能导致梯度消失

**解决方案：缩放点积**
$$e_i = \frac{q \cdot X_i}{\sqrt{D_X}}$$

这种缩放使架构更通用、更可扩展地适应不同维度向量，为不同维度向量提供更平滑的softmax梯度流动。

### 泛化2：处理多个查询向量

我们希望能够**同时处理一组查询向量**，并行处理。

**新的形状**：
- 查询向量：$Q$ [$N_Q \times D_Q$]，有 $N_Q$ 个查询向量
- 数据向量：$X$ [$N_X \times D_X$]

**计算**：

1. **相似度**：
   $$E = \frac{QX^T}{\sqrt{D_X}} \quad [N_Q \times N_X]$$
   其中 $E_{ij} = \frac{Q_i \cdot X_j}{\sqrt{D_X}}$
   
   如何高效计算所有查询与所有数据向量之间的点积？**矩阵乘法**！因为矩阵乘法输出的每个元素就是一个行与一个列的内积。

2. **注意力权重**：
   $$A = \text{softmax}(E, \text{dim}=1) \quad [N_Q \times N_X]$$
   对每一列进行softmax，每个查询预测一个数据向量上的分布

3. **输出向量**：
   $$Y = AX \quad [N_Q \times D_X]$$
   其中 $Y_i = \sum_j A_{ij} X_j$
   
   这也是矩阵乘法！计算数据向量的线性组合。

### 泛化3：分离键（Key）和值（Value）

在之前的方程中，**数据向量 $X$ 在计算中出现两次**：
1. 用于计算与查询向量的相似度
2. 用于计算输出向量（作为线性组合的对象）

在两种不同上下文中重复使用数据向量似乎有点奇怪。我们将引入**键（Key）和值（Value）**的概念，分离数据向量的这两种用途。

**新的机制**：
- 为每个数据向量，将其投影为两个向量：
  - **键向量（Key vector）**：与查询向量比较以计算对齐分数
  - **值向量（Value vector）**：要进行线性组合的对象
- 添加两个可学习的权重矩阵：
  - **键矩阵**：$W_K$ [$D_X \times D_Q$]
  - **值矩阵**：$W_V$ [$D_X \times D_V$]

**完整计算**：

**输入**：
- 查询向量：$Q$ [$N_Q \times D_Q$]
- 数据向量：$X$ [$N_X \times D_X$]
- 键矩阵：$W_K$ [$D_X \times D_Q$]
- 值矩阵：$W_V$ [$D_X \times D_V$]

**计算**：
1. **键**：$K = XW_K$ [$N_X \times D_Q$]
2. **值**：$V = XW_V$ [$N_X \times D_V$]
3. **相似度**：$E = \frac{QK^T}{\sqrt{D_Q}}$ [$N_Q \times N_X$]，其中 $E_{ij} = \frac{Q_i \cdot K_j}{\sqrt{D_Q}}$
4. **注意力权重**：$A = \text{softmax}(E, \text{dim}=1)$ [$N_Q \times N_X$]
5. **输出**：$Y = AV$ [$N_Q \times D_V$]，其中 $Y_i = \sum_j A_{ij} V_j$

**直觉**：
- 这类似于搜索引擎：要将查询与答案分开
- 你输入查询"世界上最好的学校是什么"
- 查询需要匹配后端的**键**
- 但从该查询中要返回的**值**是"斯坦福"，这是一个不同的值
- 我们分离了数据向量的两个用途：匹配（键）和检索（值）

**所有这些矩阵都是可学习的参数**，通过梯度下降学习，模型将自行学习如何分别投影到键和值，以适应问题需求。

### 交叉注意力层（Cross-Attention Layer）

这就是我们的**注意力操作符**，独立于循环神经网络：
- 一个独立的神经网络层
- 接收两个输入：查询向量和数据向量
- 包含两个可学习参数：键矩阵和值矩阵
- 输入两个向量序列，输出一个向量序列

这被称为**交叉注意力层**，因为它有两个不同的输入集同时进来。核心思想是我们同时拥有数据向量和查询向量，它们可能来自两个不同的来源。每个查询需要从数据中提取信息进行总结。

## 自注意力层（Self-Attention Layer）

还存在另一种更常见的版本：**自注意力层（Self-Attention Layer）**。

接下来我们要做的是：我们只有**一个输入集合**，只有一个输入序列，只有一个向量集合。数据向量和查询向量不再分离。

### 自注意力的定义

**输入**：
- 输入向量：$X$ [$N \times D_{in}$]
- 键矩阵：$W_K$ [$D_{in} \times D_{out}$]
- 值矩阵：$W_V$ [$D_{in} \times D_{out}$]
- 查询矩阵：$W_Q$ [$D_{in} \times D_{out}$]

**计算**：
1. **查询**：$Q = XW_Q$ [$N \times D_{out}$]
2. **键**：$K = XW_K$ [$N \times D_{out}$]
3. **值**：$V = XW_V$ [$N \times D_{out}$]
4. **相似度**：$E = \frac{QK^T}{\sqrt{D_Q}}$ [$N \times N$]，其中 $E_{ij} = \frac{Q_i \cdot K_j}{\sqrt{D_Q}}$
5. **注意力权重**：$A = \text{softmax}(E, \text{dim}=1)$ [$N \times N$]
6. **输出**：$Y = AV$ [$N \times D_{out}$]，其中 $Y_i = \sum_j A_{ij} V_j$

**关键特点**：
- 从**每个输入**计算一个查询、键和值向量
- 每个输入产生一个输出，输出是所有输入信息的混合
- 形状通常更简单：$N$ 个输入向量，每个 $D_{in}$维；几乎总是 $D_Q = D_V = D_{out}$

**实践技巧**：
通常融合为一次矩阵乘法：
$$[Q \; K \; V] = X[W_Q \; W_K \; W_V]$$
$$[N \times 3D_{out}] = [N \times D_{in}][D_{in} \times 3D_{out}]$$

因为硬件上较少的大矩阵乘法比多次小矩阵乘法更高效。

### 自注意力的置换等变性

考虑打乱输入的情况：

如果我们打乱输入 $X_1, X_2, X_3$ 的顺序（例如变成 $X_3, X_1, X_2$）：
- 查询、键和值将相同但被打乱
- 相似度分数相同但被打乱
- 注意力权重相同但被打乱
- 输出相同但被打乱

这意味着：**自注意力是置换等变的（Permutation Equivariant）**
$$F(\sigma(X)) = \sigma(F(X))$$

这意味着**自注意力实际上作用于向量集合（sets of vectors）**，而非向量序列。该层的计算不依赖我们呈现输入的顺序。

### 位置编码（Positional Encoding）

**问题**：自注意力不知道序列的顺序！

**解决方案**：在每个输入向量上附加额外的数据片段，称为**位置编码（Positional Encoding）**，用于告知神经网络每个向量的索引位置。

位置编码是一个向量，是索引的固定函数：$E(1), E(2), E(3), ...$

### 掩码自注意力（Masked Self-Attention）

在完整的自注意力中，允许输入的每个部分查看其他所有部分。但对于某些问题，我们可能需要在计算中施加结构限制。

**掩码机制**：
- 在计算对齐分数后，将需要阻断的位置覆盖为 $-\infty$
- 经过softmax后，$-\infty$ 变为0
- 这意味着输出 $Y$ 将不会依赖于该索引处的值向量

**应用**：语言建模
- 不让向量在序列中"向前看"
- 第一个输出只依赖第一个单词
- 第二个输出只依赖前两个单词
- 以此类推

示例：预测"Attention is very cool"
- 用"Attention"预测"is"
- 用"Attention is"预测"very"
- 用"Attention is very"预测"cool"

掩码矩阵：
$$
E = \begin{bmatrix}
E_{11} & -\infty & -\infty \\
E_{21} & E_{22} & -\infty \\
E_{31} & E_{32} & E_{33}
\end{bmatrix}
$$

经过softmax后对应位置的注意力权重为0。

### 多头自注意力（Multi-Head Self-Attention）

**核心思想**：并行运行 $H$ 个独立的自注意力副本。

**为什么**：
- 计算量更大，浮点运算更多
- 参数更多
- 深度学习总是想要更多更大
- 这是增强网络的一种方式，使这一层更强大更复杂

**机制**：
1. 将输入 $X$ 路由到 $H$ 个独立的自注意力层
2. 每个生成自己的输出 $Y$
3. 这些输出沿输出维度堆叠
4. 在输出端进行线性投影以融合来自各独立头的输出数据

**形状**：
- 输入：$X$ [$N \times D$]
- 权重：$W_K, W_V, W_Q$ [$D \times HD_H$]，$W_O$ [$HD_H \times D$]
- 每个头使用维度 $D_H$ =  "head dim"
- 通常 $D_H = D/H$，使输入和输出维度相同

**计算**：
1. **QKV投影**：$Q = XW_Q, K = XW_K, V = XW_V$ [每个为 $H \times N \times D_H$]
2. **相似度**：$E = \frac{QK^T}{\sqrt{D_Q}}$ [$H \times N \times N$]
3. **注意力权重**：$A = \text{softmax}(E, \text{dim}=2)$ [$H \times N \times N$]
4. **头输出**：$Y = AV$ [$H \times N \times D_H$] => [$N \times HD_H$]
5. **输出投影**：$O = YW_O$ [$N \times D$]

**实践**：
- 可以通过批量矩阵乘法并行计算所有 $H$ 个头
- 如果巧妙使用批量矩阵乘法，整个自注意力操作只是**四次矩阵乘法**！

### 自注意力的四次矩阵乘法

整个自注意力操作本质上只是**四个大规模矩阵乘法**：

1. **QKV投影**：
   $$[N \times D] \times [D \times 3HD_H] \Rightarrow [N \times 3HD_H]$$
   分割并重塑得到 $Q, K, V$，每个形状为 $[H \times N \times D_H]$

2. **QK相似度**：
   $$[H \times N \times D_H] \times [H \times D_H \times N] \Rightarrow [H \times N \times N]$$

3. **V加权**：
   $$[H \times N \times N] \times [H \times N \times D_H] \Rightarrow [H \times N \times D_H]$$
   重塑为 $[N \times HD_H]$

4. **输出投影**：
   $$[N \times HD_H] \times [HD_H \times D] \Rightarrow [N \times D]$$

**复杂度分析**：
- **计算复杂度**：$O(N^2)$（随序列长度）
- **内存复杂度**：$O(N^2)$

如果 $N=100K$，$H=64$，则 $H \times N \times N$ 的注意力权重需要1.192 TB！GPU没有那么多内存...

**Flash Attention**：
- [Dao et al, "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", 2022]
- Flash Attention算法在不存储完整注意力矩阵的情况下，同时计算步骤2+3
- 使大规模 $N$ 成为可能
- **内存复杂度降为** $O(N)$

## 三种序列处理方法对比

### 1. 循环神经网络（RNN）
- 作用于**一维有序序列**
- **(+)** 理论上适合长序列：序列长度 $N$ 的计算和内存复杂度为 $O(N)$
- **(-)** 不可并行化：需要顺序计算隐藏状态

### 2. 卷积（Convolution）
- 作用于 **$N$ 维网格**
- **(+)** 可并行化：输出可以并行计算
- **(-)** 不适合长序列：需要堆叠多层才能构建大感受野

### 3. 自注意力（Self-Attention）
- 作用于**向量集合**
- **(+)** 适合长序列：每个输出直接依赖所有输入
- **(+)** 高度并行：只是4次矩阵乘法
- **(-)** 昂贵：序列长度 $N$ 的计算复杂度为 $O(N^2)$，内存复杂度为 $O(N)$（使用Flash Attention）

**结论**：**Attention is All You Need** [Vaswani et al, NeurIPS 2017]

## Transformer架构

### Transformer块的组成

**输入**：向量集合 $x_1, x_2, x_3, x_4$

**Transformer Block包含**：

1. **（多头）自注意力层**
   - 所有向量通过自注意力相互交互
   
2. **残差连接（Residual Connection）**
   - 将自注意力输出加到原输入上：$x + \text{SelfAttention}(x)$
   
3. **层归一化（Layer Normalization）**
   - 归一化所有向量
   - 回忆Layer Norm：给定 $h_1, ..., h_N$（形状：$D$），缩放：$\gamma$（形状：$D$），偏移：$\beta$（形状：$D$）
   - $\mu_i = (\sum_j h_{i,j})/D$（标量）
   - $\sigma_i = (\sum_j (h_{i,j} - \mu_i)^2/D)^{1/2}$（标量）
   - $z_i = (h_i - \mu_i)/\sigma_i$
   - $y_i = \gamma * z_i + \beta$
   
4. **MLP/FFN（多层感知机/前馈网络）**
   - 独立作用于每个向量
   - 通常是两层MLP：$D \Rightarrow 4D \Rightarrow D$
   - 让网络能够独立处理向量，逐个处理
   
5. **另一个残差连接**
   
6. **另一个层归一化**

**输出**：向量集合 $y_1, y_2, y_3, y_4$

**关键特点**：
- **自注意力是向量之间唯一的交互**
- LayerNorm和MLP独立作用于每个向量
- 高度可扩展和可并行化
- 大部分计算只是**6次矩阵乘法**：
  - 4次来自自注意力
  - 2次来自MLP

### Transformer就是Transformer块的堆叠

**Transformer就是多个相同Transformer块的堆叠！**

自2017年以来架构变化不大...但规模变大了很多：

- **原始** [Vaswani et al, 2017]：12个块，$D=1024$，$H=16$，$N=512$，**213M参数**
- **GPT-2** [Radford et al, 2019]：48个块，$D=1600$，$H=25$，$N=1024$，**1.5B参数**
- **GPT-3** [Brown et al, 2020]：96个块，$D=12288$，$H=96$，$N=2048$，**175B参数**

同一架构在计算能力、规模和参数上跨越了多个数量级！

## Transformer的应用

### 1. 语言建模（LLM）

**示例输入**："Attention is all you"

**步骤**：

1. **嵌入矩阵（Embedding Matrix）**：[$V \times D$]
   - 在模型开始时学习嵌入矩阵，将单词转换为向量
   - 给定词汇表大小 $V$ 和模型维度 $D$，这是一个形状为 $[V \times D]$ 的查找表

2. **使用掩码注意力**
   - 在每个Transformer块内使用掩码注意力
   - 每个token只能看到它之前的token

3. **投影矩阵（Projection Matrix）**：[$D \times V$]
   - 在结尾，学习一个投影矩阵
   - 将每个 $D$ 维向量投影到 $V$ 维向量
   - 得到词汇表中每个元素的分数

4. **训练**
   - 使用softmax + 交叉熵损失
   - 训练预测下一个token

**输出**："is all you need"

### 2. 视觉Transformer（ViT）

[Dosovitskiy et al, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", ICLR 2021]

**步骤**：

1. **输入图像**：例如 $224 \times 224 \times 3$

2. **分割为patches**：例如 $16 \times 16 \times 3$

3. **展平并应用线性变换**：$768 \Rightarrow D$
   - 等价于：16×16卷积，stride=16，3个输入通道，$D$ 个输出通道

4. **每个patch得到一个 $D$ 维向量**
   - 这些是Transformer的输入向量

5. **使用位置编码**
   - 告诉Transformer每个patch的2D位置

6. **不使用掩码**
   - 每个图像patch可以查看所有其他图像patch

7. **Transformer输出每个patch的向量**

8. **池化（Pooling）**
   - 平均池化 $N \times D$ 向量到 $1 \times D$
   - 应用线性层 $D \Rightarrow C$ 预测类别分数

## Transformer的改进

自2017年以来架构变化不大，但有几个改进变得常见：

### 1. Pre-Norm Transformer

[Baevski & Auli, "Adaptive Input Representations for Neural Language Modeling", arXiv 2018]

- **原始**：层归一化在残差连接外部，模型无法真正学习恒等函数
- **改进**：将层归一化移到自注意力和MLP之前，在残差连接内部
- **好处**：训练更稳定

### 2. RMSNorm

[Zhang and Sennrich, "Root Mean Square Layer Normalization", NeurIPS 2019]

用**根均方归一化（RMSNorm）**替代层归一化：

- 输入：$x$（形状 $D$）
- 输出：$y$（形状 $D$）
- 权重：$\gamma$（形状 $D$）

$$y_i = \frac{x_i}{RMS(x)} * \gamma_i$$

$$RMS(x) = \sqrt{\epsilon + \frac{1}{N}\sum_{i=1}^N x_i^2}$$

**好处**：训练更稳定

### 3. SwiGLU MLP

[Shazeer, "GLU Variants Improve Transformers", 2020]

**经典MLP**：
- 输入：$X$ [$N \times D$]
- 权重：$W_1$ [$D \times 4D$]，$W_2$ [$4D \times D$]
- 输出：$Y = \sigma(XW_1)W_2$ [$N \times D$]

**SwiGLU MLP**：
- 输入：$X$ [$N \times D$]
- 权重：$W_1, W_2$ [$D \times H$]，$W_3$ [$H \times D$]
- 输出：$Y = (\sigma(XW_1) \odot XW_2)W_3$
- 设置 $H = 8D/3$ 保持相同总参数量

**引用**："We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence."（我们无法解释为何这些架构有效；我们将其成功归因于神的恩赐。）

### 4. 专家混合（Mixture of Experts, MoE）

[Shazeer et al, "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", 2017]

**机制**：
- 在每个块中学习 $E$ 个独立的MLP权重集；每个MLP是一个专家
- 权重：$W_1$：[$D \times 4D$] => [$E \times D \times 4D$]
- 权重：$W_2$：[$4D \times D$] => [$E \times 4D \times D$]
- 每个token被路由到 $A < E$ 个专家（活跃专家）
- **参数增加** $E$ **倍，但计算只增加** $A$ **倍**

**应用**：
- 所有当今最大的LLM（如GPT-4o、GPT-4.5、Claude 3.7、Gemini 2.5 Pro等）几乎肯定使用MoE
- 拥有 >1T 参数
- 但它们不再公布细节

## 总结

### 注意力机制
- 一种新的操作向量集合的原始方法
- 高度并行化，本质上只是几个矩阵乘法
- 高度可扩展、高度灵活
- 可应用于多种不同场景

### Transformer
- 使用自注意力作为主要计算单元的神经网络架构
- **Transformer基本上是当今深度学习所有应用都在使用的架构**
- 非常强大、非常有趣、非常令人兴奋
- Transformer已经陪伴我们八年了，看不到它们很快被淘汰

### 应用领域
- **语言**：语言建模、机器翻译
- **视觉**：图像分类、图像生成
- **语音**：音频处理
- 以及更多...

Transformer是所有大型AI模型的骨干！

---

**下次课程**：检测、分割与可视化
