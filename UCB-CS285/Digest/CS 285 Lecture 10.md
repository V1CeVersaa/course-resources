# CS 285 第 10 讲：最优控制与规划（Optimal Control and Planning）

欢迎来到 CS 285 的第 10 讲。今天我们要“换挡”：从无模型的强化学习算法，转到那些实际利用模型的算法。但在详细进入“基于模型的强化学习”（model-based RL）之前，我们需要先搞清楚一件事——无论模型是通过学习得到的，还是事先手动指定的，我们究竟如何用“模型”来做决策。因此，这一讲我们聚焦一类不直接涉及“学习”的方法：最优控制与规划。它们假设可以访问系统的已知模型，并据此做出决策；与前九讲中介绍的无模型方法相比，这些算法看起来会非常不同。也正因为如此，今天的讲座实际上不会出现“学习”步骤；不过在后续的讲座里，我们会把今天的这些最优控制与规划方法与“学到的模型”结合，从而更好地做决策。

本讲将先简要介绍基于模型的强化学习；接着讨论在已知动力学的前提下如何做决策；再谈一些非常简单、因此被广泛使用的随机黑箱优化方法；随后介绍蒙特卡罗树搜索（MCTS）；最后进入轨迹优化，特别是线性二次调节器（LQR）及其非线性扩展。我们的目标是理解：在离散与连续空间中，已知动力学模型时如何做规划；并对广泛使用的最优控制与轨迹优化算法形成整体把握。

## 回顾：强化学习的目标

在此前的讲座中，我们学习了围绕强化学习目标进行优化的多种算法。标准目标是最大化策略诱导的轨迹分布下的期望回报：
$$
\max_{\theta}\; J(\theta)
= \mathbb{E}_{\tau \sim p_\theta(\tau)}\!\left[ \sum_{t=0}^{T-1} r(s_t,a_t) \right],
$$
其中轨迹
$
\tau=(s_0,a_0,s_1,a_1,\dots,s_T),
$
而轨迹分布由初始状态分布、策略以及环境转移共同决定：
$$
p_\theta(\tau)\;=\;p(s_0)\;\prod_{t=0}^{T-1}\;\pi_\theta(a_t\mid s_t)\;p(s_{t+1}\mid s_t,a_t).
$$
为强调对参数的依赖，也常写作“由策略 $\pi_\theta$ 诱导的轨迹分布”。

在我们此前讨论的无模型设定中，假设不知道转移概率 $p(s_{t+1}\mid s_t,a_t)$，并且也不尝试去学习它。尽管如此，这些算法仍然可以在与环境交互、采样整条轨迹的过程中优化目标，而无需显式知道每一步的转移概率，也无需预测“如果在同一状态选择另一个动作会怎样”。回想我们对 Q-learning 的讨论：通过转向 $Q$ 函数，我们有意绕开了对动态模型的需求。

## 如果我们知道转移动力学，会更容易吗？

很多问题里，我们确实已经知道（或足够准确地掌握）动力学：
- 游戏与棋类（Atari、国际象棋、围棋）：规则就是动力学，要么在程序里写死，要么清楚地写在规则书中。
- 易于手工建模的系统：例如在光滑、干燥、无打滑的路面上开车，车辆的运动学方程可以直接写出，通常是很好的近似。
- 仿真环境：在模拟器中，转移过程由代码实现（尽管若模拟器极其复杂，某些便利量如导数未必有简洁的闭式表达）。

另一些场景虽然不直接“已知”动力学，但“容易学”：
- 系统辨识（system identification）：在已知模型结构的前提下，拟合未知参数。比如四足机器人腿长已知，但质量与电机扭矩未知，就以这些量为参数进行辨识。
- 通用建模：用灵活的函数逼近器直接拟合 $p(s_{t+1}\mid s_t,a_t)$ 的条件分布。这将是许多基于模型的 RL 算法的核心。

知道动力学是否会让事情更容易？通常是的。一旦掌握了动力学，我们的工具箱里就多出了一批强力算法——其中很多都属于最优控制与规划范畴——而这些在无模型设定下是无法直接使用的。稍后我会展示它们能有多强大。

## 基于模型的 RL 与本讲定位

所谓基于模型的强化学习，是指先学习转移动力学，然后利用学得的模型来选择动作。今天我们暂时不谈“如何学习模型”，而是聚焦“已知模型时如何决策”。“已知模型”意味着：在 MDP 的图模型中，从 $(s_t,a_t)$ 指向 $s_{t+1}$ 的那条边——也就是 $p(s_{t+1}\mid s_t,a_t)$——我们完全知道；离散情形下是条件概率表，连续情形下是条件分布的函数形式；确定性情形则是确定的动力学函数。

在术语上，“轨迹优化”（trajectory optimization）通常指在连续状态—动作空间里选择一段状态—动作轨迹以优化某个结果；“规划”（planning）常被用作其离散版本的说法（当然也可推广到连续版本，此时两者几乎等价）。两者的实现思路常有差异：以“规划”为名的方法往往在离散分支的树里枚举多种可能，而“轨迹优化”常更偏向光滑的、梯度型的优化。“最优控制”（optimal control）则更为宽泛：选择控制量以最大化回报或最小化代价。轨迹优化可以被视为求解最优控制问题的一条途径。从某种意义上说，整个强化学习都可以看作是从“学习”的角度在求解最优控制。

今天我们完全处在“动力学已知”的设定里；下周再讨论“动力学未知时如何学习它”；再往后还会讨论如何进一步“学习策略”（例如模仿最优控制的行为）。今天的主题是：在没有显式学习策略的前提下，如何直接求得近似最优的动作。

## 规划/控制的目标：从“老虎”说起

当我们直接做规划或控制时，关注的不再是“学习一个在所有状态上通用的策略”，而是“从当前起点出发，选择一段动作使目标最优”。一个形象的例子是“遇到老虎”：合理的规划目标就是选择一段动作，使“被老虎吃掉”的概率最小。这就是一个纯粹的规划问题——你只关心选哪些动作，不关心是否得到一个可泛化到所有状态的策略。

形式化地，在确定性动力学下可以写成带约束的优化问题（用代价最小化表述）：
$$
\begin{aligned}
\min_{a_{0:T-1}}\quad & \sum_{t=0}^{T-1} c(s_t,a_t) \\
\text{s.t.}\quad & s_{t+1} = f(s_t,a_t),\quad t=0,\dots,T-1,\\
& s_0\ \text{给定或服从已知分布。}
\end{aligned}
$$
若用回报最大化，等价地写成
$$
\max_{a_{0:T-1}} \sum_{t=0}^{T-1} r(s_t,a_t)
\quad \text{s.t.}\quad s_{t+1}=f(s_t,a_t).
$$
注：讲义某页有个小笔误，把应为 $s_{t+1}$ 的地方误写成了 $a_{t+1}$，以这里的式子为准。

## 确定性情形：开环规划

确定性情形相对直接。环境报告当前状态 $s_0$，智能体在“模型里”做一次优化，找出一段动作 $a_{0:T-1}$ 来最小化总代价或最大化总回报；随后将这段动作“发回环境”并依次执行。这里的 $a_{0:T-1}$ 就是一个“计划”（plan）。在模型正确且没有不确定性的前提下，如此得到的开环计划可以实现最优行为。

## 随机情形：开环规划及其局限

在随机动力学下，给定一段动作序列 $a_{0:T-1}$，状态序列的分布为
$$
p(s_{0:T}\mid a_{0:T-1}) \;=\; p(s_0)\prod_{t=0}^{T-1} p(s_{t+1}\mid s_t,a_t).
$$
注意这里没有 $\pi(a_t\mid s_t)$，因为我们把动作固定为“计划”。此时的开环规划可写为
$$
\max_{a_{0:T-1}}\;
\mathbb{E}_{s_{0:T}\sim p(\cdot\mid a_{0:T-1})}\!\left[\sum_{t=0}^{T-1} r(s_t,a_t)\right].
$$
这在形式上无可厚非，但在很多随机问题中会非常次优。关键在于“信息揭示”：如果未来会揭示对决策有用的信息，而你在计划时又无法利用这些未来信息来调整动作，那么仅凭事先承诺的一段动作就可能错失大量价值。

一个贴切的比喻是“算术小测验”。你也许对所有可能出现的算术题都胸有成竹，但在看到试题之前，你没法预先声明“我将依次写下的具体答案序列”。如果强行让你现在提交一串“答案动作”，你会发现对于大多数可能的试卷，这串答案都很离谱，于是“拒绝考试”反而成了期望回报更高（至少不扣更多分）的“开环最优解”。然而，一旦允许你在看到题目后再作答（即利用观测到的未来状态），结果就截然不同——这正是闭环的力量。

## 术语旁注：什么是“开环/闭环”？

所谓“闭环”，指感知—决策—执行的回路被闭合：智能体在每一步先观察状态 $s_t$，再据此选择动作 $a_t$，通常由某个策略 $\pi(a_t\mid s_t)$ 实现；随后进入下一步观察—决策—执行的循环。“开环”则是只在 $t=0$ 观察一次状态，随即一次性交付整段动作序列 $a_{0:T-1}$ 并按序执行；之后不再根据新观测做任何调整。可以形象地把开环理解为：状态只在 $t=0$ 被“发送”过来一次，此后就是单向通信。

在简单的确定性场景中，开环可能足够好；但在有不确定性且未来会揭示有用信息的场景中，开环通常次优，我们更希望采用闭环规划。

## 随机情形：闭环规划与局部策略

闭环规划把“提交动作序列”升级为“提交一个将状态映射到动作的关系”，也就是提交一个策略 $\pi(a_t\mid s_t)$。目标回到标准 RL 的形式：
$$
\max_{\pi}\; \mathbb{E}_{\tau\sim p^\pi(\tau)}\!\left[\sum_{t=0}^{T-1} r(s_t,a_t)\right].
$$
与此前课程中讨论的神经网络等“全局策略”不同，在很多最优控制应用里，一个在起点附近有效的“局部策略”就足以接近最优。比如控制火箭沿预定轨迹飞行，尽管存在扰动（风、气流、推力噪声），若扰动不大且能及时校正，系统状态会始终围绕计划轨迹的小邻域波动，此时一个简单的时变线性反馈控制器便能很好地工作：
$$
a_t \;=\; K_t\,\bigl(s_t - \bar{s}_t\bigr) + k_t,
$$
其中 $\bar{s}_t$ 是参考轨迹上的期望状态，$K_t$、$k_t$ 随时间变化并在规划时求得。这样的“局部闭环”策略，是最优控制与轨迹优化（如 LQR、iLQR/DDP）中的常见形态：它不试图在全状态空间给出统一决策，而是专注于从起点出发、沿计划轨迹附近实现高质量的反馈控制。

---

总结：
- 本讲从无模型 RL 过渡到“已知模型”下的最优控制与规划，今天不涉及“学习”，后续再把“学到的模型”与这些方法结合。
- 强化学习的标准目标是最大化策略诱导的轨迹期望回报：$J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}[\sum r(s_t,a_t)]$，其中 $p_\theta(\tau)=p(s_0)\prod \pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)$。
- 在确定性情形下，规划是带动力学约束的优化：$\min_{a_{0:T-1}}\sum c(s_t,a_t)\ \text{s.t.}\ s_{t+1}=f(s_t,a_t)$；讲义中关于 $s_{t+1}$ 的笔误已在文中更正。
- 随机情形下的开环规划无法利用未来揭示的信息，往往次优；更合理的是闭环规划，即直 接优化策略 $\pi(a_t\mid s_t)$。
- 实践中常用“局部闭环策略”，如时变线性反馈，为后续的 LQR 及其非线性扩展（iLQR/DDP）奠定了概念基础。

# CS 285 第 10 讲（Part 2）：开环规划的黑箱优化与离散情形下的 MCTS

在接下来的这一段，我们先讨论一类对动力学模型要求极低的“开环规划”方法。它们确实需要你能在模型里“滚动”一次轨迹、拿到回报，但除此之外不关心模型是离散还是连续、确定还是随机、可微还是不可微。我们暂时把问题固定为开环：给定当前状态，从模型中评估“整段动作序列”的好坏，从而选出一段能让回报最大的序列。用它去做“数学考试”那样需要根据中途信息调整答案的任务当然不合适，但在很多实际场景里，它们既简单又管用。

为便于叙述，先把时间结构抽象掉。把整段动作写作
$
\mathbf{A} \equiv (a_1, a_2, \dots, a_T),
$
目标记作
$
J(\mathbf{A}),
$
把它理解为在模型中以这段动作展开轨迹得到的期望回报。接下来要做的事，本质上就是一个“黑箱”最优化：给定可评估的目标 J，我们要在变量向量 A 上最大化 J(A)。

## 随机射击法（random shooting）：猜测—检验

最朴素也最容易落地的一招，是“猜测—检验”。做法是从某个分布中独立采样 N 段动作序列
$
\{\mathbf{A}^{(i)}\}_{i=1}^N \sim p_0(\mathbf{A})
$
（可以是简单的均匀分布），对每一段都在模型里评估一次 J，然后取
$
\mathbf{A}^\star \in \arg\max_{i=1,\dots,N} J\!\left(\mathbf{A}^{(i)}\right).
$
看起来粗糙，但对低维系统和较短的时间地平线，实际效果往往不错，而且它有几个非常现实的优点：实现几乎不费工夫；当模型是神经网络时，能把 N 段动作当成一个“迷你批”并行前向，GPU 上一把跑完，最后做一个 max-reduction 就行。缺点同样直观：靠“抽到好样本”的运气，如果行动维度或时域太大，中奖概率迅速下降。

## 交叉熵方法（CEM）：在好样本附近收缩分布

如何在保留“并行易实现”的优点同时提升成功率？一个经典改进是交叉熵方法（cross-entropy method, CEM）。核心思想是“有针对性地采样”：不是一直用同一个无偏分布，而是在每一轮根据“表现最好的一小撮样本”（精英样本）重拟合采样分布，反复迭代、逐步把概率质量聚焦到“看起来有希望”的区域。

一个常见的连续动作实现流程如下：
1) 采样与评估：从当前分布 p(𝐀; θ) 采样 {𝐀(i)}Ni=1，计算 {J(𝐀(i))}。
2) 选取精英：取前 m 个高分样本组成精英集 E（例如 m≈0.1N）。
3) 重拟合分布：把 θ 更新为能最大似然拟合精英集的参数
   $
   \theta^{\text{new}} \in \arg\max_\theta \sum_{\mathbf{A}\in E}\log p(\mathbf{A};\theta).
   $
   若 p 取高斯族，就得到“用精英的均值—协方差作新 μ、Σ”的规则。
4) 迭代以上步骤，直至预算（轮数/评估次数）用完，输出当前最优样本或分布的均值作为计划。

直觉图景是：先撒点“四面八方”的样本；看到哪儿有高地，就把分布往那儿挪；再撒、更聚焦；循环往复，逐步爬上山顶。CEM 的优点包括：对模型—动作是否可微不敏感；能天然并行；易于换成适合离散动作的分布族（如分类分布）；在样本覆盖足够大时具有到达全局最优的“极限”性质（尽管理论所需样本/轮数在复杂问题上会非常大）。在实践中，还常见一些“带动量”的扩展（例如 CMA-ES），在多轮迭代、小种群的设置下往往更稳。

小结一下这类“黑箱开环规划”的利弊。优点是：实现极简，便于在 GPU 上高度并行；对模型与动作的可微性没有要求。主要瓶颈在维度：经验上，超过大约 30–60 个“有效自由度”就会开始吃力。注意“有效”二字——虽然 T 步、每步 d 维总自由度是 dT，但相邻时间步往往强相关，例如 d=10、T=15 的情形未必就完全失效，但再大就要小心了。另一个限制是：它们产出的只是开环序列，无法在执行中利用新信息修正动作。

## 离散情形的闭环规划：蒙特卡罗树搜索（MCTS）

现在换个思路，看看如何在规划中显式考虑“闭环反馈”。蒙特卡罗树搜索（Monte Carlo Tree Search, MCTS）在离散状态/动作问题特别常用（棋类、牌类、许多电子游戏），也能用于连续情形的离散化近似。

直观做法是：从根节点（当前状态）出发，沿“动作”展开一棵搜索树。如果把每条可能路径都扩到深度 T，再把每个叶子都完整评估一遍，确实能找到最优动作，但代价指数级膨胀，不现实。MCTS 的关键是“部分展开 + 随机回滚（rollout）”：把树只扩到有限深度 D，然后在叶子节点上用一个默认策略（哪怕是随机策略）继续往下滚到终止，从而得到该叶子状态的一个“蒙特卡罗估计的价值”。虽然不是精确值，但只要展开得足够、默认策略不至于太离谱，估计仍然有参考意义：非常差的状态，随机滚动也大概率得分差；非常好的胜势，随便下也不容易输。

在随机环境下，同一条从根到叶的“动作序列”多次执行可能落到不同状态，因此对同一叶子需要做多次回滚取期望的样本估计。算法每轮会把回滚得到的回报沿路径向上“回传”，更新每个经过节点的累计价值与访问次数；然后继续选择下一个要扩展的叶子。实际执行时，通常每个时间步都重新跑一次 MCTS：根随环境推进而变化。

要在“继续深入有希望的分支”与“去探索未知分支”之间权衡，MCTS 常配一个“树策略（tree policy）”来挑叶子。最常见的是 UCT（Upper Confidence bounds applied to Trees），其在已完全展开的节点处按以下打分选子节点 i：

$$
\mathrm{score}(i)
= \underbrace{\frac{Q_i}{N_i}}_{\text{当前平均价值}} + c\,\sqrt{\frac{2\ln N_{\text{parent}}}{N_i}},
$$

其中 Q_i 是到目前为止沿该子节点累计的回报和，N_i 是访问次数，N_parent 是父节点被访问的总次数，c>0 控制“探索”强度。若某个动作还未被展开过，先展开它；否则选 score 最大的子节点递归向下。每次完成一次回滚，就把路径上每个节点的统计量做

$
Q \leftarrow Q + R,\quad N \leftarrow N + 1
$

的更新，平均价值即 Q/N。

讲个简化的执行片段来把感觉固化一下。根处两个动作 a1=0 与 a1=1，一开始各取一次回滚，得到 +10 与 +15。别忘了这是随机样本估计，下一次可能不是这两个数。此时 a1=1 的平均值高，但 a1=0 的访问次数更少。在 UCT 的打分里，a1=0 会拿到更大的“探索奖励”，因此即便它当前均值低，也可能被优先再次访问。不断迭代“选叶—展开—回滚—回传”后，按照计算预算停下，从根选平均回报最高的动作去执行。下一步进入环境返回的新状态，再从这个新根重复上述过程。

从实用角度说，MCTS 在“存在随机性、需要闭环评估”的离散问题上表现很强；它的理论保证并不多，但工程上非常好用。还可以做许多增强：用你当前最好的策略去当默认策略以获得更高质量的回滚；在叶子处用价值函数近似器做终止评估；更激进一些，把 MCTS 与策略/价值学习联合训练，形成类似 AlphaGo 那样的组合。

---

总结：
- 开环黑箱优化把整段动作视作变量 𝐀，直接最大化 J(𝐀)。随机射击法极简易并行但靠抽样运气；CEM 通过“精英重拟合”迭代收缩采样分布，显著提高效率，仍保持黑箱与并行友好。
- 这类方法的瓶颈是维度与仅产开环解：经验上 30–60 个有效自由度之后效果显著下降；无法在执行中利用新观测纠偏。
- 要考虑闭环反馈，MCTS 用“部分展开 + rollout + 上置信界（UCT）”在树上做探索—利用权衡，更新节点的累计价值 Q 与访问次数 N，实践中对离散且具随机性的规划非常有效，可与学得的策略/价值函数相结合进一步提升表现。

# CS 285 第 10 讲（Part 3）：带导数的轨迹优化与线性二次调节器（LQR）

到目前为止，我们已经介绍了两类不依赖导数的规划方法：黑箱随机优化（随机射击、CEM）以及在离散情形下能显式考虑闭环反馈的 MCTS。接下来我们进入“带导数的轨迹优化”。相比前述方法，这里我们将利用模型的导数信息来更高效、数值上更稳定地优化整段轨迹。

说明一下记号：在强化学习/动态规划中我们习惯用状态—动作记号 $(s_t,a_t)$ 并最大化回报 $r$；而在最优控制社区更常用 $(x_t,u_t)$ 并以代价（cost）最小化为目标。接下来的推导将采用 $(x_t,u_t)$ 与代价记号，但你可以在心里把 $x\!\leftrightarrow\! s$、$u\!\leftrightarrow\! a$ 做替换，把“最小化代价”看作“最大化负代价（回报）”，二者仅差一个符号。

## 从约束优化到可微的无约束目标

基于模型的规划在确定性情形下可写为带动力学约束的优化：
$$
\begin{aligned}
\min_{u_{0:T-1}} \quad & \sum_{t=0}^{T-1} c_t(x_t,u_t) \\
\text{s.t.}\quad & x_{t+1}=f_t(x_t,u_t),\quad t=0,\dots,T-1.
\end{aligned}
$$

这类“等式约束”可以通过代入逐步消去，将约束吸收到目标里，得到一个仅关于控制序列的无约束问题：
$$
\min_{u_{0:T-1}}\; c_0(x_0,u_0)
+ c_1\bigl(f_0(x_0,u_0),u_1\bigr)
+ \cdots
+ c_{T-1}\!\Bigl(\underbrace{f_{T-2}(\cdots f_0(x_0,u_0),u_{T-2})}_{x_{T-1}},u_{T-1}\Bigr).
$$

如果我们希望用梯度法直接优化，就必须能求出链式法则所需的导数：
$$
\frac{\partial f_t}{\partial x_t},\quad
\frac{\partial f_t}{\partial u_t},\quad
\frac{\partial c_t}{\partial x_t},\quad
\frac{\partial c_t}{\partial u_t}.
$$
但在实践中，仅用一阶梯度下降（甚至动量/自适应变体）常常表现很差：原因在于长时间地平线下反复相乘的雅可比会造成梯度爆炸/消失，除非所有雅可比的特征值恰好接近 1。二阶方法（如牛顿法）可以从曲率角度进行补偿，显著改善收敛与数值稳定性。下面我们将看到，在轨迹优化这一特殊结构中，存在一类高效的“类牛顿”二阶算法，无需显式构造和求解巨大的 Hessian。

## 射击法与共置法：数值条件与变量选择

理解两种常见范式有助于把握算法行为。

射击法（shooting）直接把优化变量选为整段控制 $u_{0:T-1}$，状态由动力学“向前滚动”得到。直观上，早期的控制会“把系统射向”某个状态区域，因此初期动作对末端结果的敏感度极高；与之相对，临近末端的动作对目标影响较小。这种“敏感度不均衡”会导致目标的数值条件很差：Hessian 同时含有很大的和很小的特征值，一阶方法往往步伐受限、震荡或停滞。

共置法（collocation）将“状态与控制”都作为优化变量，并通过等式约束强制动力学一致（也可只优化状态，由约束反解控制，近似等价于“逆动力学”）。这通常改善了数值条件，因为任何一个变量的微小调整不再沿整条动力学链放大。但共置的约束实现（精确/松弛/线性化）与求解器细节较多，实现复杂度也更高。今天我们不展开共置，而是专注一种极为经典的“二阶射击法”：线性二次调节器（LQR）。

## 线性情形：LQR 的推导

为展示二阶方法的结构，我们从线性动力学与二次代价出发，随后再讨论如何走向非线性。

设引入堆叠向量
$
z_t \triangleq \begin{bmatrix}x_t\\ u_t\end{bmatrix}.
$
动力学为时变线性、确定性：
$$
x_{t+1}=F_t z_t + f_t,
$$
其中 $F_t$ 是把 $(x_t,u_t)$ 映到 $x_{t+1}$ 的矩阵，$f_t$ 是常向量。单步代价为二次型（允许含一次项与常数项，但常数项不影响最优策略）：
$$
c_t(x_t,u_t)
= \tfrac12 z_t^\top C_t z_t + z_t^\top c_t.
$$
将 $C_t$ 与 $c_t$ 按块写为
$
C_t=
\begin{bmatrix}
C_{xx,t} & C_{xu,t}\\
C_{ux,t} & C_{uu,t}
\end{bmatrix},
\quad
c_t=
\begin{bmatrix}
c_{x,t}\\
c_{u,t}
\end{bmatrix},
$
并假设 $C_t$ 对称（故 $C_{ux,t}=C_{xu,t}^\top$）。注意若代价仅线性，最小化会“冲向无穷”，因此二次项是必要的，也通常要求 $C_{uu,t}\succ 0$ 以确保对控制的强凸性。

我们采用“从后往前”的二次型值函数参数化。记“值函数”为
$$
V_t(x)\;=\;\tfrac12\,x^\top V_t x \;+\; x^\top v_t \;+\; \text{const},
$$
终止条件（或“末端值”）取
$
V_T(x)=\tfrac12 x^\top V_T x + x^\top v_T
$
或若无末端代价，则 $V_T\equiv 0$。在时刻 $t$ 定义“$Q$ 函数”（当前一步动作 + 之后最优）：
$$
Q_t(x_t,u_t)\;=\; c_t(x_t,u_t)\;+\; V_{t+1}\!\bigl(F_t z_t+f_t\bigr).
$$
由于动力学线性、$V_{t+1}$ 二次，$Q_t$ 对 $(x_t,u_t)$ 仍是二次型，可写为
$$
Q_t(x,u)
=\tfrac12
\begin{bmatrix}x\\ u\end{bmatrix}^\top
\!\begin{bmatrix}Q_{xx,t} & Q_{xu,t}\\ Q_{ux,t} & Q_{uu,t}\end{bmatrix}
\begin{bmatrix}x\\ u\end{bmatrix}
+\begin{bmatrix}x\\ u\end{bmatrix}^\top
\begin{bmatrix}q_{x,t}\\ q_{u,t}\end{bmatrix}.
$$

将 $Q_t$ 展开并“按公式凑块”，能得到极其简洁的组合关系：二次项由动力学“左右各乘一次”地把下一步的曲率传回，线性项由动力学把一次项与平移项回传：
$$
\begin{aligned}
\begin{bmatrix}Q_{xx,t} & Q_{xu,t}\\ Q_{ux,t} & Q_{uu,t}\end{bmatrix}
&=
C_t \;+\; F_t^\top V_{t+1} F_t,\\[4pt]
\begin{bmatrix}q_{x,t}\\ q_{u,t}\end{bmatrix}
&=
c_t \;+\; F_t^\top\!\bigl(V_{t+1} f_t + v_{t+1}\bigr).
\end{aligned}
$$
直观地，“$V$ 的二次项经动力学映射回来叠加到当前步代价的二次项上”，而“一次项则既受到 $V$ 的一次项回传，也受到 $V$ 的二次项对平移 $f_t$ 的影响”。

接下来对 $Q_t$ 关于 $u$ 求极小，得到时变线性反馈律
$$
u_t^\star \;=\; K_t x_t + k_t,
\qquad
K_t \;=\; -\,Q_{uu,t}^{-1} Q_{ux,t},\quad
k_t \;=\; -\,Q_{uu,t}^{-1} q_{u,t}.
$$
将最优 $u_t^\star$ 代回 $Q_t$，即可得到仅关于 $x_t$ 的值函数参数递推（离散型 Riccati 回代）：
$$
\begin{aligned}
V_t
&= Q_{xx,t} - Q_{xu,t}\,Q_{uu,t}^{-1}\,Q_{ux,t},\\[2pt]
v_t
&= q_{x,t} - Q_{xu,t}\,Q_{uu,t}^{-1}\,q_{u,t}.
\end{aligned}
$$
由于我们关心的是策略与相对值，常数项可忽略；若需轨迹的绝对代价，可同步累加常数。

把上述几步连起来，就得到 LQR 的“后向—前向”两段式算法：后向递推计算时变增益与值函数，前向滚动得到最优轨迹。

后向递推（$t=T-1,\dots,0$）：
1) 用 $V_{t+1},v_{t+1}$ 形成 $Q$ 的块矩阵与向量（上式）。
2) 由 $Q_{uu,t},Q_{ux,t},q_{u,t}$ 得到反馈律 $K_t,k_t$。
3) 代回得到 $V_t,v_t$。

前向滚动（$t=0,\dots,T-1$）：
$$
u_t = K_t x_t + k_t,\qquad x_{t+1}=F_t\!\begin{bmatrix}x_t\\ u_t\end{bmatrix}+f_t.
$$
这里的 $Q_t(x_t,u_t)$ 正是“现在在 $x_t$ 采取 $u_t$，随后按最优策略行动的总剩余代价”；而 $V_t(x_t)$ 是“现在在 $x_t$ 开始并随后按最优策略行动的总剩余代价”。

## 为什么二阶射击法在这里高效且稳定？

表面上看，LQR 是线性—二次的“特殊情况”。但其关键是结构：后向把曲率通过线性动力学“传回”，每一步只需解一个与控制维度同阶的线性方程组（$Q_{uu,t}$ 的逆），而不需要构造/反演整条轨迹的巨型 Hessian。于是我们在“利用二阶信息”的同时，避免了数值与计算的灾难。相比之下，直接对无约束目标做一阶反向传播，由于长链雅可比的乘积，极易出现梯度爆炸/消失；而 LQR 的 Riccati 回代等价于对该长链的“预条件/重加权”，本质上就是一个结构化的二阶步骤。

## 从线性到非线性：为 iLQR/DDP 铺路

虽然本节聚焦线性情形，但它直接指向非线性系统的经典做法：在参考轨迹附近把动力学 $f_t$ 线性化、把代价 $c_t$ 二次化（即二阶泰勒展开），从而在每次迭代上解一个“局部 LQR 子问题”，得到时变线性反馈律，再前向滚动更新参考轨迹并线性化/二次化，循环迭代。这就是 DDP/iLQR 的核心思想，下一部分我们将系统展开。

在动手推导时，强烈建议你拿一张纸，亲自把“$Q$ 的块结构”“$K,k$ 的求解”“$V,v$ 的回代”逐步写出来。它们在线性代数上非常直接，只是书写较长；一旦自己走通一次，LQR 的结构与直觉会非常清晰。

---

总结：
- 将带动力学约束的规划问题改写为无约束目标后，直接用一阶方法常因雅可比连乘导致梯度消失/爆炸；二阶方法可显著改善数值性质。
- 射击法优化控制、共置法同时优化状态与控制；共置通常数值条件更好，但实现复杂。我们聚焦“二阶射击法”——LQR。
- 在线性动力学与二次代价下，$Q$ 函数与值函数都保持二次型；后向用
  $Q=C+F^\top V F,\; q=c+F^\top(Vf+v)$
  组合，解
  $u^\star=Kx+k$（$K=-Q_{uu}^{-1}Q_{ux},\; k=-Q_{uu}^{-1}q_u$），再得
  $V=Q_{xx}-Q_{xu}Q_{uu}^{-1}Q_{ux},\; v=q_x-Q_{xu}Q_{uu}^{-1}q_u$。
- 前向据 $u_t=K_t x_t+k_t$ 与动力学滚动出最优轨迹。该结构化“类牛顿”步骤避免构造巨型 Hessian，为后续的 iLQR/DDP（非线性情形）提供了直接模板。

# CS 285 第 10 讲（Part 4）：随机动力学与非线性系统——LQG、iLQR/DDP 与线搜索

在上一部分，我们完成了线性—二次情形下的 LQR 推导。接下来把结论拓展到两类更一般的设定：一是线性高斯的随机动力学（LQG）；二是非线性系统，通过局部线性化与二次化得到迭代 LQR/DDP。最后，我们讨论为何需要在前向滚动时做“回溯/线搜索”，以及如何具体实施。

## 一、线性高斯随机动力学：LQG

把确定性的线性动力学
$$
x_{t+1}=F_t \begin{bmatrix}x_t\\ u_t\end{bmatrix}+f_t
$$
推广到线性高斯随机情形：
$$
p(x_{t+1}\mid x_t,u_t)=\mathcal{N}\!\left(
F_t\begin{bmatrix}x_t\\ u_t\end{bmatrix}+f_t,\;\Sigma_t\right),
$$
且单步代价仍为二次型
$$
c_t(x_t,u_t)=\tfrac12
\begin{bmatrix}x_t\\ u_t\end{bmatrix}^{\!\top}
C_t
\begin{bmatrix}x_t\\ u_t\end{bmatrix}
+\begin{bmatrix}x_t\\ u_t\end{bmatrix}^{\!\top}c_t.
$$

关键事实：在这种“线性动力学 + 二次代价 + 高斯噪声”的设定下，最优的闭环控制律与确定性 LQR 完全相同，仍为时变线性反馈
$$
u_t^\star=K_t\,x_t+k_t,
$$
其中的 $K_t,k_t$ 由与确定性 LQR 完全一致的后向递推得到。高斯噪声不会改变最优增益的表达式，直觉是“二次函数在高斯下的期望仍是二次”，噪声只改变常数项，不影响对 $u_t$ 的一阶条件。不同之处在于：状态序列不再是单一路径，而是一个高斯过程；因此产出的是闭环策略（控制律），而非一条开环动作序列。这正是 LQG 的经典结论。

结论要点：
- 算法形式不变，后向递推与确定性 LQR 相同；
- 前向时 $x_t$ 来自随机滚动，但执行的控制律 $u_t=K_t x_t+k_t$ 即为闭环最优；
- 在线性—高斯—二次设定下，LQR 自然“升级”为 LQG。

## 二、非线性系统：从 LQR 到 iLQR 与 DDP

现实系统往往是非线性的：$x_{t+1}=f(x_t,u_t)$，$c_t=c(x_t,u_t)$。能否把它“局部近似”为线性—二次问题并用 LQR 解子问题？答案是可以，通过泰勒展开在一条参考（名义）轨迹附近做线性化与二次化，这就得到 iLQR（iterative LQR）；若进一步把动力学也做二阶展开并保留二阶项，则得到 DDP（differential dynamic programming）。

设当前名义轨迹为 $\{\hat{x}_t,\hat{u}_t\}_{t=0}^{T-1}$，定义偏差
$
\delta x_t=x_t-\hat{x}_t,\quad \delta u_t=u_t-\hat{u}_t.
$
对动力学做一阶线性化：
$$
x_{t+1}\approx \hat{x}_{t+1}+A_t\,\delta x_t+B_t\,\delta u_t,
\quad
A_t=\left.\frac{\partial f}{\partial x}\right|_{(\hat{x}_t,\hat{u}_t)},\;
B_t=\left.\frac{\partial f}{\partial u}\right|_{(\hat{x}_t,\hat{u}_t)}.
$$
对代价做二阶展开：
$$
\begin{aligned}
c_t(x_t,u_t)
&\approx \hat{c}_t+ c_{x,t}^\top\delta x_t + c_{u,t}^\top\delta u_t \\
&\quad + \tfrac12
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}^{\!\top}
\begin{bmatrix} 
C_{xx,t} & C_{xu,t}\\
C_{ux,t} & C_{uu,t}
\end{bmatrix}
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix},
\end{aligned}
$$
其中 $c_{x,t},c_{u,t},C_{\cdot\cdot,t}$ 为在 $(\hat{x}_t,\hat{u}_t)$ 处的一阶/二阶偏导。

在这组“线性化动力学 + 二次化代价”的近似系统上，直接套用上一部分的 LQR 后向—前向两段式，得到偏差空间的最优反馈律
$$
\delta u_t = k_t + K_t\,\delta x_t.
$$
把它还原到原变量就是
$$
u_t \;=\; \hat{u}_t \;+\; k_t \;+\; K_t\,(x_t-\hat{x}_t).
$$

iLQR 的一次迭代包含：
- 后向传递（用线性化/二次化模型）求 $K_t,k_t$；
- 前向滚动时不再用线性化动力学，而是用原始非线性 $f$，并用上式控制律产生新轨迹 $\{x_t,u_t\}$；
- 用新轨迹更新名义轨迹 $(\hat{x}_t,\hat{u}_t)\leftarrow(x_t,u_t)$，再进入下一轮线性化与二次化；
- 迭代直至收敛。

这本质上是把原问题在名义轨迹邻域做“牛顿式”近似，解一个结构化的二次子问题（LQR），不断迭代逼近局部极值。若仅对动力学取一阶（上式的 $A_t,B_t$），相当于对原问题进行了高斯—牛顿近似，这就是 iLQR；若进一步保留动力学的二阶导（需要三维张量，如 $f_{xx},f_{xu},f_{uu}$），就得到严格意义上的牛顿步，对应 DDP。两者都保留了“值函数二次、Q 函数二次、Riccati 回代”的优雅结构，只是 DDP 的推导中会出现张量项。

## 三、为何需要线搜索（回溯）？如何实施？

纯牛顿法/高斯—牛顿法的“全步更新”在非凸、强非线性问题上经常失效：二次近似只在名义轨迹附近可靠，若一步走太远，真实代价可能上升。iLQR 中一个简单有效的做法是在前向滚动时对“前馈项”做缩放线搜索，保持反馈项不变：
$$
u_t \;=\; \hat{u}_t \;+\; \alpha\,k_t \;+\; K_t\,(x_t-\hat{x}_t),\qquad \alpha\in(0,1].
$$
当 $\alpha\to 0$ 时，控制退化为沿用旧的名义动作（因为 $x_1=\hat{x}_1$，继而 $x_2\approx\hat{x}_2$，…），从而“回到起点附近”。实践中可采用以下简洁流程：
- 固定本轮求得的 $K_t,k_t$，在若干 $\alpha$ 备选值上做前向滚动，用原始 $f$ 计算真实总代价；
- 若代价未下降，则减小 $\alpha$ 继续尝试，直到代价下降为止；
- 更讲究的做法是用二次近似预测“期望下降量”，要求真实下降量至少达到其某个比例；也可做简单的区间（bracketing）线搜索。

这一步等价于为“二次近似的可信域”做隐式调节，使每次更新既不过猛、也不保守。

补充说明：在许多实现里，还会对 $Q_{uu,t}$ 施加 LM 式阻尼（Levenberg–Marquardt），或等价地对值函数曲率做正则化，以增强数值稳健性；但就本讲核心流程而言，前向线搜索已能显著改善收敛表现。

---

总结：
- 在线性—高斯—二次设定（LQG）下，最优闭环控制律与确定性 LQR 相同，噪声不改变 $K_t,k_t$ 的求解，只把结果从“开环序列”提升为“闭环策略”。
- 对非线性系统，围绕名义轨迹做一阶线性化（动力学）与二阶二次化（代价），即可用 LQR 解出偏差空间的线性反馈律，前向用真动力学滚动并更新名义轨迹，迭代至收敛（iLQR）。若进一步保留动力学二阶项，则得到 DDP，可视作更“完整”的牛顿步。
- 为保证每步改进，前向滚动时对前馈项做线搜索：$u_t=\hat{u}_t+\alpha k_t+K_t(x_t-\hat{x}_t)$，逐步减小 $\alpha$ 至代价下降，或达到预测下降的一定比例。
